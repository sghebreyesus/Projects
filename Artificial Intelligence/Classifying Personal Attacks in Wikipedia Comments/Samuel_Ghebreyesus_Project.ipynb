{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wikipedia Talk Data - Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook gives an introduction to working with the various data sets in [Wikipedia\n",
    "Talk](https://figshare.com/projects/Wikipedia_Talk/16731) project on Figshare. The release includes:\n",
    "\n",
    "1. a large historical corpus of discussion comments on Wikipedia talk pages\n",
    "2. a sample of over 100k comments with human labels for whether the comment contains a personal attack\n",
    "3. a sample of over 100k comments with human labels for whether the comment has aggressive tone\n",
    "\n",
    "Please refer to our [wiki](https://meta.wikimedia.org/wiki/Research:Detox/Data_Release) for documentation of the schema of each data set and our [research paper](https://arxiv.org/abs/1610.08914) for documentation on the data collection and modeling methodology. \n",
    "\n",
    "In this notebook we show how to build a simple classifier for detecting personal attacks and apply the classifier to a random sample of the comment corpus to see whether discussions on user pages have more personal attacks than discussion on article pages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Building a classifier for personal attacks\n",
    "In this section we will train a simple bag-of-words classifier for personal attacks using the [Wikipedia Talk Labels: Personal Attacks]() data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import metrics\n",
    "pd.options.display.max_colwidth = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download annotated comments and annotations\n",
    "\n",
    "ANNOTATED_COMMENTS_URL = 'https://ndownloader.figshare.com/files/7554634' \n",
    "ANNOTATIONS_URL = 'https://ndownloader.figshare.com/files/7554637' \n",
    "\n",
    "def download_file(url, fname):\n",
    "    urllib.request.urlretrieve(url, fname)\n",
    "\n",
    "# You can edit the code here to download only once, and not download it later                \n",
    "download_file(ANNOTATED_COMMENTS_URL, 'attack_annotated_comments.tsv')\n",
    "download_file(ANNOTATIONS_URL, 'attack_annotations.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = pd.read_csv('attack_annotated_comments.tsv', sep = '\\t', index_col = 0)\n",
    "annotations = pd.read_csv('attack_annotations.tsv',  sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "115864"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['rev_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels a comment as an atack if the majority of annotators did so\n",
    "labels = annotations.groupby('rev_id')['attack'].mean() > 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join labels and comments\n",
    "comments['attack'] = labels\n",
    "#print (df)\n",
    "\n",
    "#below code was used to filter the dataframe according to various conditions so I could find out the percentage attack/\n",
    "#nonattack depending on ns, logged_in and sample values. I used the data found here to create visualizations in excel \n",
    "filterinfDataframe = comments[(comments['ns'] == 'user') & (comments['attack'] == True)]\n",
    "filterinfDataframe\n",
    "filterinfDataframetwo = comments[(comments['logged_in'] == True) & (comments['attack'] == True)]\n",
    "filterinfDataframetwo\n",
    "comments['logged_in'].value_counts()\n",
    "filterinfDataframethree = comments[(comments['sample'] == 'blocked') & (comments['attack'] == True)]\n",
    "#filterinfDataframethree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions a-k\n",
    "a. From my visualizations, I learned that approximately 88% of the comments in the dataset are not attacks while 12% are\n",
    "(13590/115864). I also learned that attacks are more likely to be found when the ns value is equal to user versus when\n",
    "it is equal to article (17.6% of user ns have attacks compared to 4.4% of article). I also saw that when logged_in is equal\n",
    "to false, the percentage of attacks is about 3.5x higher than when it is equal to true (24.7% vs 7%). I also saw that when\n",
    "sample is equal to blocked, the percentage of attacks is higher than when it is equal to random (16.9% vs 0.9%). Lastly,\n",
    "when looking at year, I saw that in the earliest years of the dataset (from about 2001-2004), the percentage of attacks is \n",
    "miniscule (ranging from 0-2%) and after those years, it remains pretty consistent between 10-13%.\n",
    "\n",
    "b. For text cleaning methods, I tried removing words that were within parentheses, lowercased all the words, removed\n",
    "punctuation, removed apostrophes, removed stop words and removed other miscellaneous symbols (i.e. equal signs, dashes, etc.) I saw when going over the comment column. I included all of these in the final code. \n",
    "\n",
    "c. The features I considered using were ns, year, logged_in and sample. I also considered adding columns to the dataset showing the number of characters in each comment and the number of exclamation points since I saw that attack comments seemed to be shorter and use a lot of exclamation points when examining the comments but when I tried that it had no effect on my f1 score so I got rid of/stopped using it. For the final code, I used the ns and logged_in features as well as comment as I saw a very slight improvement in f1 score using those features. \n",
    "\n",
    "d. I simply labeled comments as attacks if the mean of the attack values was greater than 0.5. I tried modifying the values to see if there were better thresholds but when I lowered it, the f1 scores would tend to go down and when I increased it, there was not much effect so I stuck with 0.5. \n",
    "\n",
    "e. I did not add any special optimizations to my code. \n",
    "\n",
    "f. The ML methods I tried out were LinearSVC, MultinomialNB and RandomForestClassifier. Without hyperparameter tuning, the results for my LinearSVC were macro average 0.84 for precision, 0.86 for recall and 0.85 for f1 score. For MultinomialNB, my results were 0.88 for precision, 0.79 for recall and 0.83 for f1 score. For RandomForestClassifier, my results were 0.90 for precision, 0.81 for recall and 0.84 for f1 score. The best ML method was LinearSVC, as it had the highest f1 score. \n",
    "\n",
    "g. For hyperparameter tuning, I created a param_grid that included loss (with two possible values: squared_hinge and hinge), C (with five possible values: 0.1, 1, 10, 100, 1000), class_weight(with 8 possible values: balanced, 1:2, 1:3, 1:4, 1:5, 1:10, 1:20 and 1:50) to help deal with imbalanced data and max_iter (with 3 possible values: 10, 100, 1000). I also used random oversampling of the minority class in order to deal with the imbalanced data and put that with the columntransform and linearsvc model that were in the pipeline. My f1 score went up 2 percentage points as a result of my hyperparameter tuning (precision went up 5 percentage points, recall went down 1 percentage point). \n",
    "\n",
    "h. From the different metrics, I learned that each of the models had pretty similar macro averaged precisions (between 0.88-0.90) but there was a wider gap in the range of macro averaged recalls (between 0.79-0.85) and that difference helped create the difference in the f1 score. Cross validation was very useful because it allows us to estimate the skill of our machine learning models on unseen data and generalizability is important in determining how effective our model truly is. The more generalizable the models, the less we might have to deal with problems in our data like overfitting.     \n",
    "\n",
    "i. My best final result metrics were macro average 0.89 precision, 0.85 recall and 0.87 f1 score. Compared to the strawman code, this is a 5 percentage point increase in f1 score, a 7 percentage point increase in precision and a 4 percentage point increase in recall. LinearSVC gave me this performance. \n",
    "\n",
    "j. The most interesting thing I learned from this project was how to approach a machine learning project from beginning to end, from observing the data and possible trends, thinking about the most effective ways to clean text, how to do feature extraction/choose features, using pipelines and column transformers and doing hyperparameter tuning. This was my first time doing a project of this nature and it was cool to play around with all these steps and try to find an optimal solution. \n",
    "\n",
    "k. The hardest thing was getting the columntransformer and pipelines working as I kept getting some error messages but eventually they were figured out. Also, it was difficult to select features and do hyperparameter tuning because it felt like no matter what features I choose or what tuning I did, there was not significant improvements in f1 score (seemed to top out at 0.87, which was 2 percentage points better than without hyperparameter tuning and 5 percentage points higher than strawman but I felt like it should have been higher). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>year</th>\n",
       "      <th>logged_in</th>\n",
       "      <th>ns</th>\n",
       "      <th>sample</th>\n",
       "      <th>split</th>\n",
       "      <th>attack</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rev_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>37675</th>\n",
       "      <td>creative dictionary definitions terms insurance ensurance properly applied destruction understand fine legitimate criticism ill write three man cell bounty hunter easy understand ensured insured different differ assured sentence quote absolutely neutral familiar underlying theory strikeback disservice reader someone comes research topic like mad want context beyond history history book fine history book claimed</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44816</th>\n",
       "      <td>term standard model less npov think wed prefer newage speak lot oldage people speak karl popper pope etc heres karl poppers view clearest title article would particle physics cosmology say would require broader treatment issues like anthropic principle cognitive bias beyond particle physics zoo etc accelerators clear use someone still looking particles yet settled cosmology certain abandon search arbitrary foundation ontology suggest subject question</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49851</th>\n",
       "      <td>true false situation march 2002 saudi proposal land peace recognition arab countries made day proposal made formal arab league day israelis command ariel sharon began invasion palestinian selfrule areas userarab</td>\n",
       "      <td>2002</td>\n",
       "      <td>False</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89320</th>\n",
       "      <td>next maybe could work less condescending suggestions reading naming conventions fdl read quite ago thanks really liked bit explaining interest fixing things complained felt insulted yet extremely insulting time luck learn less jerk greglindahl</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>dev</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93890</th>\n",
       "      <td>page need disambiguation</td>\n",
       "      <td>2002</td>\n",
       "      <td>True</td>\n",
       "      <td>article</td>\n",
       "      <td>random</td>\n",
       "      <td>train</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                       comment  \\\n",
       "rev_id                                                                                                                                                                                                                                                                                                                                                                                                                                                                           \n",
       "37675                                           creative dictionary definitions terms insurance ensurance properly applied destruction understand fine legitimate criticism ill write three man cell bounty hunter easy understand ensured insured different differ assured sentence quote absolutely neutral familiar underlying theory strikeback disservice reader someone comes research topic like mad want context beyond history history book fine history book claimed   \n",
       "44816   term standard model less npov think wed prefer newage speak lot oldage people speak karl popper pope etc heres karl poppers view clearest title article would particle physics cosmology say would require broader treatment issues like anthropic principle cognitive bias beyond particle physics zoo etc accelerators clear use someone still looking particles yet settled cosmology certain abandon search arbitrary foundation ontology suggest subject question   \n",
       "49851                                                                                                                                                                                                                                                      true false situation march 2002 saudi proposal land peace recognition arab countries made day proposal made formal arab league day israelis command ariel sharon began invasion palestinian selfrule areas userarab   \n",
       "89320                                                                                                                                                                                                                      next maybe could work less condescending suggestions reading naming conventions fdl read quite ago thanks really liked bit explaining interest fixing things complained felt insulted yet extremely insulting time luck learn less jerk greglindahl   \n",
       "93890                                                                                                                                                                                                                                                                                                                                                                                                                                                 page need disambiguation   \n",
       "\n",
       "        year  logged_in       ns  sample  split  attack  \n",
       "rev_id                                                   \n",
       "37675   2002      False  article  random  train   False  \n",
       "44816   2002      False  article  random  train   False  \n",
       "49851   2002      False  article  random  train   False  \n",
       "89320   2002       True  article  random    dev   False  \n",
       "93890   2002       True  article  random  train   False  "
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove newline and tab tokens\n",
    "\n",
    "# did some additional text cleanup here, removed words that were within parentheses, lowercased all the words, removed\n",
    "# punctuation, removed apostrophes, removed stop words and removed other miscellaneous symbols I saw when going over the \n",
    "#comment column\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"=\", \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace('`', \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(':', \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace('<', \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace('>', \"\"))\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.replace(\"-\", \"\"))\n",
    "comments['comment'] = comments['comment'].str.replace('[^\\w\\s]','')\n",
    "comments['comment'] = comments['comment'].str.replace(r\"\\(.*\\)\",\"\")\n",
    "comments['comment'] = comments['comment'].apply(lambda x: x.lower())\n",
    "comments['comment'] = comments['comment'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop)]))\n",
    "comments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rev_id\n",
       "801279                                                                                                                                                                                                                                                                                                                                     iraq good usa bad\n",
       "2702703    ____ fuck little asshole want talk human start showing fear way humans act around humans continue beligerant campaign cross another boundary begin offsite recruitmehnt escalate till rhetorically nuclear whole goddamed mob think find want better start expressing interest concerns presented credibility either document community pile shit\n",
       "4632658                                                                                                                                                                                                                                                                                                                                   dick bigger hahaha\n",
       "6545332                                                                                                                                                                                                                                                                              renault sad little bpy driving renault clio vaa voom mcflurry made shit\n",
       "6545351                                                                                                                                                                                                                                                                               renault sad little bo driving renault clio vaa voom mcflurry made shit\n",
       "Name: comment, dtype: object"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comments.query('attack')['comment'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.96      0.96     20422\n",
      "        True       0.69      0.66      0.67      2756\n",
      "\n",
      "    accuracy                           0.92     23178\n",
      "   macro avg       0.82      0.81      0.82     23178\n",
      "weighted avg       0.92      0.92      0.92     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fit a simple text classifier\n",
    "\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', DecisionTreeClassifier(random_state = 123)),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "met = metrics.classification_report(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98339  3935]\n",
      " [ 1321 12269]]\n"
     ]
    }
   ],
   "source": [
    "#print out confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = comments.attack\n",
    "y_pred = grid.predict(comments)\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Method 1: LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_validate\n",
    "from imblearn.over_sampling  import RandomOverSampler\n",
    "t = [('cat', OneHotEncoder(handle_unknown='ignore'), [\"ns\"]), ('cat_two', OneHotEncoder(handle_unknown='ignore'), [\"logged_in\"]), ('comment', TfidfVectorizer(), \"comment\")]\n",
    "col_transform = ColumnTransformer(transformers=t)\n",
    "svc_pipe  = Pipeline([('prep',col_transform),\n",
    "                     ('sampler', RandomOverSampler(sampling_strategy='minority',random_state=42)),\n",
    "                     ('model',   LinearSVC())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(\n",
    "    train_df : pd.DataFrame,\n",
    "    test_df  : pd.DataFrame,\n",
    "    pipe     : Pipeline,\n",
    ") -> None:\n",
    "\n",
    "    model = pipe.fit(train_comments, \n",
    "                     train_comments[\"attack\"])\n",
    "\n",
    "\n",
    "    pred  = model.predict(test_comments)\n",
    "    print(metrics.classification_report(test_comments[\"attack\"],\n",
    "                                pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "evaluate_pipeline = partial(evaluate_model,\n",
    "                            train_comments,\n",
    "                            test_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.96      0.96     20422\n",
      "        True       0.72      0.75      0.74      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.84      0.86      0.85     23178\n",
      "weighted avg       0.94      0.94      0.94     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(svc_pipe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model__C': 1, 'model__class_weight': 'balanced', 'model__loss': 'hinge', 'model__max_iter': 10}\n",
      "Pipeline(steps=[('prep',\n",
      "                 ColumnTransformer(transformers=[('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['ns']),\n",
      "                                                 ('cat_two',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['logged_in']),\n",
      "                                                 ('comment', TfidfVectorizer(),\n",
      "                                                  'comment')])),\n",
      "                ('sampler',\n",
      "                 RandomOverSampler(random_state=42,\n",
      "                                   sampling_strategy='minority')),\n",
      "                ('model',\n",
      "                 LinearSVC(C=1, class_weight='balanced', loss='hinge',\n",
      "                           max_iter=10))])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.97      0.96      0.97     20422\n",
      "        True       0.74      0.76      0.75      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.85      0.86      0.86     23178\n",
      "weighted avg       0.94      0.94      0.94     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling  import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "t = [('cat', OneHotEncoder(handle_unknown='ignore'), [\"ns\"]), ('cat_two', OneHotEncoder(handle_unknown='ignore'), \n",
    "                                                               [\"logged_in\"]), ('comment', TfidfVectorizer(), \"comment\")]\n",
    "col_transform = ColumnTransformer(transformers=t)\n",
    "model = LinearSVC()\n",
    "svc_pipe  = Pipeline([('prep',col_transform),\n",
    "                     ('sampler', RandomOverSampler(sampling_strategy='minority',random_state=42)),\n",
    "                     ('model',   LinearSVC())])\n",
    "param_grid = {\n",
    "              'model__loss': ['squared_hinge', 'hinge'],\n",
    "              'model__C': [0.1, 1, 10, 100, 1000],\n",
    "              'model__class_weight': ['balanced', {1:2},{1:3},{1:4},{1:5},{1:10},{1:20},{1:50}],\n",
    "              'model__max_iter': [10, 100, 1000]}\n",
    "grid = GridSearchCV(svc_pipe, param_grid=param_grid, scoring='f1_macro', verbose=10, n_jobs=-1)\n",
    "grid.fit(train_comments, train_comments['attack'])\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "grid_predictions = grid.predict(test_comments)\n",
    "print(metrics.classification_report(test_comments['attack'], grid_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tuning Hyperparameters Best Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 240 candidates, totalling 1200 fits\n",
      "{'m__C': 0.1, 'm__class_weight': {1: 3}, 'm__loss': 'squared_hinge', 'm__max_iter': 100}\n",
      "Pipeline(steps=[('prep',\n",
      "                 ColumnTransformer(transformers=[('cat',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['ns']),\n",
      "                                                 ('cat_two',\n",
      "                                                  OneHotEncoder(handle_unknown='ignore'),\n",
      "                                                  ['logged_in']),\n",
      "                                                 ('comment', TfidfVectorizer(),\n",
      "                                                  'comment')])),\n",
      "                ('m', LinearSVC(C=0.1, class_weight={1: 3}, max_iter=100))])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.96      0.98      0.97     20422\n",
      "        True       0.81      0.73      0.77      2756\n",
      "\n",
      "    accuracy                           0.95     23178\n",
      "   macro avg       0.89      0.85      0.87     23178\n",
      "weighted avg       0.95      0.95      0.95     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "t = [('cat', OneHotEncoder(handle_unknown='ignore'), [\"ns\"]), ('cat_two', OneHotEncoder(handle_unknown='ignore'), [\"logged_in\"]), ('comment', TfidfVectorizer(), \"comment\")]\n",
    "col_transform = ColumnTransformer(transformers=t)\n",
    "model = LinearSVC()\n",
    "pipeline = Pipeline(steps=[('prep',col_transform), ('m', model)])\n",
    "param_grid = {\n",
    "              'm__loss': ['squared_hinge', 'hinge'],\n",
    "              'm__C': [0.1, 1, 10, 100, 1000],\n",
    "              'm__class_weight': ['balanced', {1:2},{1:3},{1:4},{1:5},{1:10},{1:20},{1:50}],\n",
    "              'm__max_iter': [10, 100, 1000]}\n",
    "grid = GridSearchCV(pipeline, param_grid=param_grid, scoring='f1_macro', verbose=10, n_jobs=-1)\n",
    "grid.fit(train_comments, train_comments['attack'])\n",
    "print(grid.best_params_)\n",
    "print(grid.best_estimator_)\n",
    "grid_predictions = grid.predict(test_comments)\n",
    "print(metrics.classification_report(test_comments['attack'], grid_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n",
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.34990966 0.46885171 0.46885171 0.46885171 0.46884885 0.46884885\n",
      " 0.46884885 0.46884885 0.34857764 0.46884885]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\samgh\\Downloads\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\_base.py:1208: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "#printing out cross_val_score for LinearSVC model, chose 10 as the value for k fold cross validation because from \n",
    "#outside research, I saw that 10 provided a good trade-off between low computational cost and low bias when estimating\n",
    "#model performance \n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "model = LinearSVC()\n",
    "y=np.array(comments.attack)\n",
    "X=comments['comment']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X = le.fit_transform(X)\n",
    "print(cross_val_score(model, X.reshape(-1, 1), y, scoring='f1_macro', cv=10, error_score='raise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[98339  3935]\n",
      " [ 1321 12269]]\n"
     ]
    }
   ],
   "source": [
    "#printing out confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = comments.attack\n",
    "y_pred = grid.predict(comments)\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Method 2: MultinomialNB\n",
    "Below are the best results from using MultinomialNB. I also tried MultinomialNB using max_features 10000 and 20000 and word analyzer with n_gram range from (1,2), (1,3) and (1,4) which led to f1 scores between 0.79 and 0.80, as the recall scores went down to about 0.73-0.74. I also tried adding various combinations of features like logged_in, ns, sample and year but that saw f1 scores decline to about 0.6-0.64 so I didn't use them.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.98      0.96     20422\n",
      "        True       0.80      0.61      0.69      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.88      0.79      0.83     23178\n",
      "weighted avg       0.93      0.94      0.93     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 30000, analyzer='char', ngram_range = (1,6))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "met = metrics.classification_report(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.46885171 0.46885171 0.46885171 0.46885171 0.46884885 0.46884885\n",
      " 0.46884885 0.46884885 0.46884885 0.46884885]\n"
     ]
    }
   ],
   "source": [
    "#printing out cross_val_score for MultinomialNB model, chose 10 as the value for k fold cross validation because from \n",
    "#outside research, I saw that 10 provided a good trade-off between low computational cost and low bias when estimating\n",
    "#model performance \n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "model = MultinomialNB()\n",
    "y=np.array(comments.attack)\n",
    "X=comments['comment']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X = le.fit_transform(X)\n",
    "print(cross_val_score(model, X.reshape(-1, 1), y, scoring='f1_macro', cv=10, error_score='raise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[99883  2391]\n",
      " [ 5018  8572]]\n"
     ]
    }
   ],
   "source": [
    "# print out confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = comments.attack\n",
    "y_pred = clf.predict(comments['comment'])\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Method 3: RandomForestClassifier\n",
    "Below are the best results from using RandomForestClassifier. I also tried changing the max_features in countvectorizer to 20000 and use n_gram ranges for words and char going from (1,2) to (1,5) as well as adding other features such as logged_in, ns, sample and year but they did not have any significant positive impact on the final metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.95      0.98      0.97     20422\n",
      "        True       0.84      0.63      0.72      2756\n",
      "\n",
      "    accuracy                           0.94     23178\n",
      "   macro avg       0.90      0.81      0.84     23178\n",
      "weighted avg       0.94      0.94      0.94     23178\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "train_comments = comments.query(\"split=='train'\")\n",
    "test_comments = comments.query(\"split=='test'\")\n",
    "\n",
    "clf = Pipeline([\n",
    "    ('vect', CountVectorizer(max_features = 10000, ngram_range = (1,2))),\n",
    "    ('tfidf', TfidfTransformer(norm = 'l2')),\n",
    "    ('clf', RandomForestClassifier()),\n",
    "])\n",
    "clf = clf.fit(train_comments['comment'], train_comments['attack'])\n",
    "\n",
    "met = metrics.classification_report(test_comments['attack'], clf.predict(test_comments['comment']))\n",
    "print(met)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.61886448 0.61760728 0.63242549 0.62684177 0.63821585 0.64709311\n",
      " 0.61750404 0.62003192 0.63015819 0.63264445]\n"
     ]
    }
   ],
   "source": [
    "#printing out cross_val_score for RandomForestClassifier model, chose 10 as the value for k fold cross validation because from \n",
    "#outside research, I saw that 10 provided a good trade-off between low computational cost and low bias when estimating\n",
    "#model performance \n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "model = RandomForestClassifier()\n",
    "y=np.array(comments.attack)\n",
    "X=comments['comment']\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "X = le.fit_transform(X)\n",
    "print(cross_val_score(model, X.reshape(-1, 1), y, scoring='f1_macro', cv=10, error_score='raise'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[101525    749]\n",
      " [  2120  11470]]\n"
     ]
    }
   ],
   "source": [
    "#print out confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = comments.attack\n",
    "y_pred = clf.predict(comments['comment'])\n",
    "print(confusion_matrix(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nice comment\n",
    "clf.predict(['Thanks for you contribution, you did a great job!'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# correctly classify nasty comment\n",
    "clf.predict(['People as stupid as you should not edit Wikipedia!'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prevalence of personal attacks by namespace\n",
    "In this section we use our classifier in conjunction with the [Wikipedia Talk Corpus](https://figshare.com/articles/Wikipedia_Talk_Corpus/4264973) to see if personal attacks are more common on user talk or article talk page discussions. In our paper we show that the model is not biased by namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from scipy.stats import bernoulli\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download and untar data\n",
    "\n",
    "USER_TALK_CORPUS_2004_URL = 'https://ndownloader.figshare.com/files/6982061'\n",
    "ARTICLE_TALK_CORPUS_2004_URL = 'https://ndownloader.figshare.com/files/7038050'\n",
    "\n",
    "download_file(USER_TALK_CORPUS_2004_URL, 'comments_user_2004.tar.gz')\n",
    "download_file(ARTICLE_TALK_CORPUS_2004_URL,  'comments_article_2004.tar.gz')\n",
    "\n",
    "os.system('tar -xzf comments_user_2004.tar.gz')\n",
    "os.system('tar -xzf comments_article_2004.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper for collecting a sample of comments for a given ns and year from \n",
    "def load_no_bot_no_admin(ns, year, prob = 0.1):\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    data_dir = \"comments_%s_%d\" % (ns, year)\n",
    "    for _, _, filenames in os.walk(data_dir):\n",
    "        for filename in filenames:\n",
    "            if re.match(\"chunk_\\d*.tsv\", filename):\n",
    "                df = pd.read_csv(os.path.join(data_dir, filename), sep = \"\\t\")\n",
    "                df['include'] = bernoulli.rvs(prob, size=df.shape[0])\n",
    "                df = df.query(\"bot == 0 and admin == 0 and include == 1\")\n",
    "                dfs.append(df)\n",
    "                \n",
    "    sample = pd.concat(dfs)\n",
    "    sample['ns'] = ns\n",
    "    sample['year'] = year\n",
    "    \n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collect a random sample of comments from 2004 for each namespace\n",
    "corpus_user = load_no_bot_no_admin('user', 2004)\n",
    "corpus_article = load_no_bot_no_admin('article', 2004)\n",
    "corpus = pd.concat([corpus_user, corpus_article])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model\n",
    "corpus['comment'] = corpus['comment'].apply(lambda x: x.replace(\"NEWLINE_TOKEN\", \" \"))\n",
    "corpus['comment'] = corpus['comment'].apply(lambda x: x.replace(\"TAB_TOKEN\", \" \"))\n",
    "corpus['attack'] = clf.predict_proba(corpus['comment'])[:,1] > 0.425 # see paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot prevalence per ns\n",
    "\n",
    "sns.pointplot(data = corpus, x = 'ns', y = 'attack')\n",
    "plt.ylabel(\"Attack fraction\")\n",
    "plt.xlabel(\"Dicussion namespace\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attacks are far more prevalent in the user talk namespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
